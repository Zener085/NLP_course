{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKboZnAdgrRM"
   },
   "source": [
    "# [NLP] Assignment 1: Tokenization\n",
    "\n",
    "In this assignment, you need to tokenize the text of the Twitter(X) users posts(tweets). The assignment consists of two tasks. When you finish all the tasks, create a GitHub repository for this assignment (you can use this repo later for the other assignments) and submit this notebook in the repository. Leave `requirements.txt` file if your code requires additional installations. Submit the link to the repository in Moodle.\n",
    "\n",
    "The [data](https://drive.google.com/file/d/15x_wPAflvYQ2Xh38iNQGrqUIWLj5l5Nw/view?usp=share_link) contains 5 files whereby each contains 44 tweets. Each tweet is separated by a newline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLDjjAvemUP_"
   },
   "source": [
    "## Task 1. Tokenize some tweets manually (20 points)\n",
    "\n",
    "As a first task you need to tokenize first 15 tweets from `file2` by hand. This will allow you to understand the problem from a linguistic point of view. The guidelines for tweet tokenization are as follows:\n",
    "\n",
    "- Each smiley is a separate token\n",
    "- Each hashtag is an individual token. Each user reference is an individual token\n",
    "- If a word has spaces between them then it is converted to a single token\n",
    "- All punctuations are individual tokens. This includes double-quotes and single quotes also\n",
    "- A URL is a single token\n",
    "\n",
    "Example of output\n",
    "\n",
    "    Input tweet\n",
    "    @xfranman Old age has made N A T O!\n",
    "\n",
    "    Tokenized tweet (separated by comma)\n",
    "    @xfranman , Old , age , has , made , NATO , !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KKKwTidnzUw"
   },
   "source": [
    "\n",
    "    1. Input tweet\n",
    "    ...\n",
    "    1. Tokenized tweet\n",
    "    ...\n",
    "\n",
    "    2. Input tweet\n",
    "    ...\n",
    "    2. Tokenized tweet\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "import regex\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T11:15:18.209001500Z",
     "start_time": "2024-02-09T11:15:13.995798600Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Camping in Maine for the weekend. Hey Dad, Mama Loves YOU: http://www.mamapalooza.com', 'Its american tradition bitch', \"@ThroughTheVoid They love it! The only pleasure they get in life. I actually do that. I'm sure I hear a tiny squeak... Then louder ones\", '\" RT @latti: @AbsoHilare stop tweeting in church! Lol <--- \"\"I tweet because I\\'m happy, I tweet because I\\'m free\"\" LOL!\"', \"Samsung Mini S2 portable HDD graced with colors that perfectly match your tacky beach gear: Sammy's done it aga.. http://tinyurl.com/lb5p6m\", \"@dialloc congrats on finding your way over. it may be slow going at first. hang in there. it's kinda cool when u get up to speed.\", 'iPhone activation delays continue, Apple offers $30 http://twt.gs/l3Ki', 'RT @GoogleAtWork Gmail maximum attachment size now 25MB http://bit.ly/62mjw Nice!!!', \"RT @acfou The Ads Won Awards for Crispin; But Did Nothing for Client BurgerKing's Sales/Marketshare - Big Surprise - http://ping.fm/vw8TI\", 'Hey doll! Great I missed True Blood yday boo lol Rt @FrankBanuat78 @jhillstephens Hello Sunshine how are u today? :-)', 'Australian artist Pogo made these free songs primarily from sampled audio from Alice In Wonderland. http://www.last.fm/music/Pogo/Wonderland', \"@mppritchard they wanted to sell all the preorders & then sell all of the ones they had in stock to those that just walked in. Can't do both\", 'Incoming: Frightened Rabbit, Sept. 22 (Tucson): If Fat Cat Records is going to send three great bands from Scot.. http://tinyurl.com/nz6xcv', 'Hey @ginoandfran please greet philip! (GinoandFran live > http://ustre.am/2YyQ)', \"Ik weet niet wie er achter de T-Mobile iPhone Twitter zit maar ik vind het niet echt 'corporate' taalgebruik... Best vreemd eigenlijk\"]\n"
     ]
    }
   ],
   "source": [
    "def extract_text(__file_name: str):\n",
    "    with open(f\"data/{__file_name}\", \"r\") as f:\n",
    "        text = f.readlines()\n",
    "    file = []\n",
    "    for line in text:\n",
    "        file.extend(line.split(\"\\n\"))\n",
    "    return file[::2]\n",
    "\n",
    "files = [extract_text(f\"file{i}\") for i in range(1, 6)]\n",
    "print(files[1][:15])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T11:15:18.276999700Z",
     "start_time": "2024-02-09T11:15:18.208015100Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "output = \"\"\"\n",
    "1. Input tweet\n",
    "Camping in Maine for the weekend. Hey Dad, Mama Loves YOU: http://www.mamapalooza.com\n",
    "1. Tokenized tweet\n",
    "Camping, in, Maine, for, the, weekend, ., Hey, Dad, ,, Mama, Loves, YOU, :, http://www.mamapalooza.com\n",
    "2. Input tweet\n",
    "Its american tradition bitch\n",
    "2. Tokenized tweet\n",
    "Its, american, tradition, bitch\n",
    "3. Input tweet\n",
    "@ThroughTheVoid They love it! The only pleasure they get in life. I actually do that. I'm sure I hear a tiny squeak... Then louder ones\n",
    "3. Tokenized tweet\n",
    "@ThroughTheVoid, They, love, it, !, The, only, pleasure, they, get, in, life, ., I, actually, do, that, ., I, ', m, sure, I, hear, a, tiny, squeak, ., ., ., Then, louder, ones\n",
    "4. Input tweet\n",
    "\" RT @latti: @AbsoHilare stop tweeting in church! Lol <--- \"\"I tweet because I'm happy, I tweet because I'm free\"\" LOL!\"\n",
    "4. Tokenized tweet\n",
    "\", RT, @latti, :, @AbsoHilare, stop, tweeting, in, church, !, Lol, <, -, -, -, \", \", I, tweet, because, I, ', m, happy, ,, I, tweet, because, I, ', m, free, \", \", LOL, !, \"\n",
    "5. Input tweet\n",
    "Samsung Mini S2 portable HDD graced with colors that perfectly match your tacky beach gear: Sammy's done it aga.. http://tinyurl.com/lb5p6m\n",
    "5. Tokenized tweet\n",
    "Samsung, Mini, S2, portable, HDD, graced, with, colors, that, perfectly, match, your, tacky, beach, gear, :, Sammy, ', s, done, it, aga, .., http://tinyurl.com/lb5p6m\n",
    "6. Input tweet\n",
    "@dialloc congrats on finding your way over. it may be slow going at first. hang in there. it's kinda cool when u get up to speed.\n",
    "6. Tokenized tweet\n",
    "@dialloc, congrats, on, finding, your, way, over, ., it, may, be, slow, going, at, first, ., hang, in, there, ., it, ', s, kinda, cool, when, u, get, up, to, speed, .\n",
    "7. Input tweet\n",
    "iPhone activation delays continue, Apple offers $30 http://twt.gs/l3Ki\n",
    "7. Tokenized tweet\n",
    "iPhone, activation, delays, continue, ,, Apple, offers, $, 30, http://twt.gs/l3Ki\n",
    "8. Input tweet\n",
    "RT @GoogleAtWork Gmail maximum attachment size now 25MB http://bit.ly/62mjw Nice!!!\n",
    "8. Tokenized tweet\n",
    "RT, @GoogleAtWork, Gmail, maximum, attachment, size, now, 25MB, http://bit.ly/62mjw, Nice, !, !, !\n",
    "9. Input tweet\n",
    "RT @acfou The Ads Won Awards for Crispin; But Did Nothing for Client BurgerKing's Sales/Marketshare - Big Surprise - http://ping.fm/vw8TI\n",
    "9. Tokenized tweet\n",
    "RT, @acfou, The, Ads, Won, Awards, for, Crispin, ;, But, Did, Nothing, for, Client, BurgerKing, ', s, Sales, /, Marketshare, -, Big, Surprise, -, http://ping.fm/vw8TI\n",
    "10. Input tweet\n",
    "Hey doll! Great I missed True Blood yday boo lol Rt @FrankBanuat78 @jhillstephens Hello Sunshine how are u today? :-)\n",
    "10. Tokenized tweet\n",
    "Hey, doll, !, Great, I, missed, True, Blood, yday, boo, lol, Rt, @FrankBanuat78, @jhillstephens, Hello, Sunshine, how, are, u, today, ?, :-)\n",
    "11. Input tweet\n",
    "Australian artist Pogo made these free songs primarily from sampled audio from Alice In Wonderland. http://www.last.fm/music/Pogo/Wonderland\n",
    "11. Tokenized tweet\n",
    "Australian, artist, Pogo, made, these, free, songs, primarily, from, sampled, audio, from, Alice, In, Wonderland, ., http://www.last.fm/music/Pogo/Wonderland\n",
    "12. Input tweet\n",
    "@mppritchard they wanted to sell all the preorders & then sell all of the ones they had in stock to those that just walked in. Can't do both\n",
    "12. Tokenized tweet\n",
    "@mppritchard, they, wanted, to, sell, all, the, preorders, &, then, sell, all, of, the, ones, they, had, in, stock, to, those, that, just, walked, in, ., Can, ', t, do, both\n",
    "13. Input tweet\n",
    "Incoming: Frightened Rabbit, Sept. 22 (Tucson): If Fat Cat Records is going to send three great bands from Scot.. http://tinyurl.com/nz6xcv\n",
    "13. Tokenized tweet\n",
    "Incoming, :, Frightened, Rabbit, ,, Sept, ., 22, (, Tucson, ), :, If, Fat, Cat, Records, is, going, to, send, three, great, bands, from, Scot, .., http://tinyurl.com/nz6xcv\n",
    "14. Input tweet\n",
    "Hey @ginoandfran please greet philip! (GinoandFran live > http://ustre.am/2YyQ)\n",
    "14. Tokenized tweet\n",
    "Hey, @ginoandfran, please, greet, philip, !, (, GinoandFran, live, >, http://ustre.am/2YyQ, )\n",
    "15. Input tweet\n",
    "Ik weet niet wie er achter de T-Mobile iPhone Twitter zit maar ik vind het niet echt 'corporate' taalgebruik... Best vreemd eigenlijk\n",
    "15. Tokenized tweet\n",
    "Ik, weet, niet, wie, er, achter, de, T, -, Mobile, iPhone, Twitter, zit, maar, ik, vind, het, niet, echt, ', corporate, ', taalgebruik, ., ., ., Best, vreemd, eigenlijk\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T11:15:18.277999500Z",
     "start_time": "2024-02-09T11:15:18.272002300Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "The First time I thought I had to create a regex system that would complete the task.\n",
    "But after understanding and finishing the task by hands, I still don't want to delete this piece of my creature.\n",
    "So I'd kindly ask you to just skip the code below up to the next task\n",
    "and don't decrease points for this task just because I also have something not related fully to the current one."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "EMOTICONS = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "      |\n",
    "      </?3                       # heart\n",
    "    )\"\"\"\n",
    "\n",
    "URLS = r\"\"\"\t\t\t# Capture 1: entire matched URL\n",
    "  (?:\n",
    "  https?:\t\t\t\t# URL protocol and colon\n",
    "    (?:\n",
    "      /{1,3}\t\t\t\t# 1-3 slashes\n",
    "      |\t\t\t\t\t#   or\n",
    "      [a-z0-9%]\t\t\t\t# Single letter or digit or '%'\n",
    "                                       # (Trying not to match e.g. \"URI::Escape\")\n",
    "    )\n",
    "    |\t\t\t\t\t#   or\n",
    "                                       # looks like domain name followed by a slash:\n",
    "    [a-z0-9.\\-]+[.]\n",
    "    (?:[a-z]{2,13})\n",
    "    /\n",
    "  )\n",
    "  (?:\t\t\t\t\t# One or more:\n",
    "    [^\\s()<>{}\\[\\]]+\t\t\t# Run of non-space, non-()<>{}[]\n",
    "    |\t\t\t\t\t#   or\n",
    "    \\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\) # balanced parens, one level deep: (...(...)...)\n",
    "    |\n",
    "    \\([^\\s]+?\\)\t\t\t\t# balanced parens, non-recursive: (...)\n",
    "  )+\n",
    "  (?:\t\t\t\t\t# End with:\n",
    "    \\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\) # balanced parens, one level deep: (...(...)...)\n",
    "    |\n",
    "    \\([^\\s]+?\\)\t\t\t\t# balanced parens, non-recursive: (...)\n",
    "    |\t\t\t\t\t#   or\n",
    "    [^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]\t# not a space or one of these punct chars\n",
    "  )\n",
    "  |\t\t\t\t\t# OR, the following to match naked domains:\n",
    "  (?:\n",
    "  \t(?<!@)\t\t\t        # not preceded by a @, avoid matching foo@_gmail.com_\n",
    "    [a-z0-9]+\n",
    "    (?:[.\\-][a-z0-9]+)*\n",
    "    [.]\n",
    "    (?:[a-z]{2,13})\n",
    "    \\b\n",
    "    /?\n",
    "    (?!@)\t\t\t        # not succeeded by a @,\n",
    "                            # avoid matching \"foo.na\" in \"foo.na@example.com\"\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "FLAGS = r\"\"\"\n",
    "  (?:\n",
    "    [\\U0001F1E6-\\U0001F1FF]{2}  # all enclosed letter pairs\n",
    "    |\n",
    "    # English flag\n",
    "    \\U0001F3F4\\U000E0067\\U000E0062\\U000E0065\\U000E006e\\U000E0067\\U000E007F\n",
    "    |\n",
    "    # Scottish flag\n",
    "    \\U0001F3F4\\U000E0067\\U000E0062\\U000E0073\\U000E0063\\U000E0074\\U000E007F\n",
    "    |\n",
    "    # For Wales? Why Richard, it profit a man nothing to give his soul for the whole world … but for Wales!\n",
    "    \\U0001F3F4\\U000E0067\\U000E0062\\U000E0077\\U000E006C\\U000E0073\\U000E007F\n",
    "  )\n",
    "\"\"\"\n",
    "\n",
    "REGEXPS = (\n",
    "    URLS,\n",
    "    EMOTICONS,\n",
    "    # HTML tags:\n",
    "    r\"\"\"<[^>\\s]+>\"\"\",\n",
    "    # ASCII Arrows\n",
    "    r\"\"\"[\\-]+>|<[\\-]+\"\"\",\n",
    "    # Twitter username:\n",
    "    r\"\"\"(?:@[\\w_]+)\"\"\",\n",
    "    # Twitter hashtags:\n",
    "    r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\",\n",
    "    # Zero-Width-Joiner and Skin tone modifier emojis\n",
    "    \"\"\".(?:\n",
    "        [\\U0001F3FB-\\U0001F3FF]?(?:\\u200d.[\\U0001F3FB-\\U0001F3FF]?)+\n",
    "        |\n",
    "        [\\U0001F3FB-\\U0001F3FF]\n",
    "    )\"\"\",\n",
    "    # flags\n",
    "    FLAGS,\n",
    "    # Remaining word types:\n",
    "    r\"\"\"\n",
    "    (?:[^\\W\\d_](?:[^\\W\\d_]|['\\-_])+[^\\W\\d_]) # Words with apostrophes or dashes.\n",
    "    |\n",
    "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
    "    |\n",
    "    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
    "    |\n",
    "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots.\n",
    "    |\n",
    "    (?:\\S)                         # Everything else that isn't whitespace.\n",
    "    \"\"\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T11:15:18.294596400Z",
     "start_time": "2024-02-09T11:15:18.277999500Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def tokenize_tweet(__tweet: str):\n",
    "    tokenizer = regex.compile(f\"({'|'.join(REGEXPS)})\", regex.VERBOSE | regex.I | regex.UNICODE)\n",
    "    words = tokenizer.findall(__tweet)\n",
    "\n",
    "    return words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T11:15:18.330535300Z",
     "start_time": "2024-02-09T11:15:18.295801900Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is how it worked."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Input tweet\n",
      "Camping in Maine for the weekend. Hey Dad, Mama Loves YOU: http://www.mamapalooza.com\n",
      "1. Tokenized tweet\n",
      "Camping, in, Maine, for, the, weekend, ., Hey, Dad, ,, Mama, Loves, YOU, :, http://www.mamapalooza.com\n",
      "2. Input tweet\n",
      "Its american tradition bitch\n",
      "2. Tokenized tweet\n",
      "Its, american, tradition, bitch\n",
      "3. Input tweet\n",
      "@ThroughTheVoid They love it! The only pleasure they get in life. I actually do that. I'm sure I hear a tiny squeak... Then louder ones\n",
      "3. Tokenized tweet\n",
      "@ThroughTheVoid, They, love, it, !, The, only, pleasure, they, get, in, life, ., I, actually, do, that, ., I'm, sure, I, hear, a, tiny, squeak, ..., Then, louder, ones\n",
      "4. Input tweet\n",
      "\" RT @latti: @AbsoHilare stop tweeting in church! Lol <--- \"\"I tweet because I'm happy, I tweet because I'm free\"\" LOL!\"\n",
      "4. Tokenized tweet\n",
      "\", RT, @latti, :, @AbsoHilare, stop, tweeting, in, church, !, Lol, <---, \", \", I, tweet, because, I'm, happy, ,, I, tweet, because, I'm, free, \", \", LOL, !, \"\n",
      "5. Input tweet\n",
      "Samsung Mini S2 portable HDD graced with colors that perfectly match your tacky beach gear: Sammy's done it aga.. http://tinyurl.com/lb5p6m\n",
      "5. Tokenized tweet\n",
      "Samsung, Mini, S2, portable, HDD, graced, with, colors, that, perfectly, match, your, tacky, beach, gear, :, Sammy's, done, it, aga, .., http://tinyurl.com/lb5p6m\n",
      "6. Input tweet\n",
      "@dialloc congrats on finding your way over. it may be slow going at first. hang in there. it's kinda cool when u get up to speed.\n",
      "6. Tokenized tweet\n",
      "@dialloc, congrats, on, finding, your, way, over, ., it, may, be, slow, going, at, first, ., hang, in, there, ., it's, kinda, cool, when, u, get, up, to, speed, .\n",
      "7. Input tweet\n",
      "iPhone activation delays continue, Apple offers $30 http://twt.gs/l3Ki\n",
      "7. Tokenized tweet\n",
      "iPhone, activation, delays, continue, ,, Apple, offers, $, 30, http://twt.gs/l3Ki\n",
      "8. Input tweet\n",
      "RT @GoogleAtWork Gmail maximum attachment size now 25MB http://bit.ly/62mjw Nice!!!\n",
      "8. Tokenized tweet\n",
      "RT, @GoogleAtWork, Gmail, maximum, attachment, size, now, 25MB, http://bit.ly/62mjw, Nice, !, !, !\n",
      "9. Input tweet\n",
      "RT @acfou The Ads Won Awards for Crispin; But Did Nothing for Client BurgerKing's Sales/Marketshare - Big Surprise - http://ping.fm/vw8TI\n",
      "9. Tokenized tweet\n",
      "RT, @acfou, The, Ads, Won, Awards, for, Crispin, ;, But, Did, Nothing, for, Client, BurgerKing's, Sales, /, Marketshare, -, Big, Surprise, -, http://ping.fm/vw8TI\n",
      "10. Input tweet\n",
      "Hey doll! Great I missed True Blood yday boo lol Rt @FrankBanuat78 @jhillstephens Hello Sunshine how are u today? :-)\n",
      "10. Tokenized tweet\n",
      "Hey, doll, !, Great, I, missed, True, Blood, yday, boo, lol, Rt, @FrankBanuat78, @jhillstephens, Hello, Sunshine, how, are, u, today, ?, :-)\n",
      "11. Input tweet\n",
      "Australian artist Pogo made these free songs primarily from sampled audio from Alice In Wonderland. http://www.last.fm/music/Pogo/Wonderland\n",
      "11. Tokenized tweet\n",
      "Australian, artist, Pogo, made, these, free, songs, primarily, from, sampled, audio, from, Alice, In, Wonderland, ., http://www.last.fm/music/Pogo/Wonderland\n",
      "12. Input tweet\n",
      "@mppritchard they wanted to sell all the preorders & then sell all of the ones they had in stock to those that just walked in. Can't do both\n",
      "12. Tokenized tweet\n",
      "@mppritchard, they, wanted, to, sell, all, the, preorders, &, then, sell, all, of, the, ones, they, had, in, stock, to, those, that, just, walked, in, ., Can't, do, both\n",
      "13. Input tweet\n",
      "Incoming: Frightened Rabbit, Sept. 22 (Tucson): If Fat Cat Records is going to send three great bands from Scot.. http://tinyurl.com/nz6xcv\n",
      "13. Tokenized tweet\n",
      "Incoming, :, Frightened, Rabbit, ,, Sept, ., 22, (, Tucson, ):, If, Fat, Cat, Records, is, going, to, send, three, great, bands, from, Scot, .., http://tinyurl.com/nz6xcv\n",
      "14. Input tweet\n",
      "Hey @ginoandfran please greet philip! (GinoandFran live > http://ustre.am/2YyQ)\n",
      "14. Tokenized tweet\n",
      "Hey, @ginoandfran, please, greet, philip, !, (, GinoandFran, live, >, http://ustre.am/2YyQ, )\n",
      "15. Input tweet\n",
      "Ik weet niet wie er achter de T-Mobile iPhone Twitter zit maar ik vind het niet echt 'corporate' taalgebruik... Best vreemd eigenlijk\n",
      "15. Tokenized tweet\n",
      "Ik, weet, niet, wie, er, achter, de, T-Mobile, iPhone, Twitter, zit, maar, ik, vind, het, niet, echt, ', corporate, ', taalgebruik, ..., Best, vreemd, eigenlijk\n"
     ]
    }
   ],
   "source": [
    "tokenized_tweets = []\n",
    "\n",
    "for i, tweet in enumerate(files[1][:15], 1):\n",
    "    print(f\"{i}. Input tweet\\n{tweet}\")\n",
    "    tokens = tokenize_tweet(tweet)\n",
    "    tokenized_tweets.append(tokens)\n",
    "    print(f\"{i}. Tokenized tweet\\n{', '.join(tokens)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T11:15:18.361700400Z",
     "start_time": "2024-02-09T11:15:18.310536400Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "It can't find any separated words such as `N A T O`. But still it shows a nice result as I see."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2J2AD2nmUhi"
   },
   "source": [
    "## Task 2. Implement [Byte-Pair Encoding(BPE)](https://arxiv.org/pdf/1508.07909.pdf) Tokenizer (80 points)\n",
    "\n",
    "### Task 2.1. Implementation (60 points)\n",
    "\n",
    "Implement the tokenizer as the BPETokenizer class:\n",
    "* Implement `train` method that learns merges and builds the vocabulary of the specified `vocab_size` (25 points).\n",
    "* Implement `tokenize` method that should tokenize the text according to the learnt merges (25 points).\n",
    "\n",
    "Your code should have docstrings and comments (10 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T12:52:24.380934100Z",
     "start_time": "2024-02-09T12:52:24.364936300Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, __vocab_size: int):\n",
    "        \"\"\"\n",
    "        Initialize BPETokenizer with the specified vocabulary size.\n",
    "\n",
    "        Parameters:\n",
    "            __vocab_size: The desired size of the vocabulary.\n",
    "        \"\"\"\n",
    "        self.vocab_size = __vocab_size\n",
    "        self.vocab = {}\n",
    "\n",
    "    def train(self, __corpus: List[str]):\n",
    "        \"\"\"\n",
    "        Train the BPETokenizer on the given corpus to learn merges and build the vocabulary.\n",
    "\n",
    "        Parameters:\n",
    "            __corpus: List of strings representing the training corpus.\n",
    "        \"\"\"\n",
    "        _char_counts = Counter(__corpus)\n",
    "        self.vocab = {char: count for char, count in _char_counts.items()}\n",
    "\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            _pairs = self._get_stats()\n",
    "            _pair = max(_pairs, key=_pairs.get)\n",
    "            \n",
    "            self._merge_vocab(_pair)\n",
    "    \n",
    "    def _merge_vocab(self, __pair):\n",
    "        _v_out = {}\n",
    "        _bigram = re.escape(\" \".join(__pair))\n",
    "        _p = re.compile(r\"(?<!\\S)\" + _bigram + r\"(?!\\S)\")\n",
    "        for _word in self.vocab:\n",
    "            _w_out = _p.sub(\"\".join(__pair), _word)\n",
    "            _v_out[_w_out] = self.vocab[_word]\n",
    "\n",
    "        self.vocab = {**_v_out, **self.vocab}\n",
    "\n",
    "    def tokenize(self, __text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize the input text according to the learned merges.\n",
    "\n",
    "        Parameters:\n",
    "            __text: The input text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            List of tokens.\n",
    "        \"\"\"\n",
    "        _tokens = []\n",
    "        _i = 0\n",
    "        while _i < len(__text):\n",
    "            for _j in range(len(__text), _i, -1):\n",
    "                _token = __text[_i:_j]\n",
    "                if _token in self.vocab:\n",
    "                    _tokens.append(_token)\n",
    "                    _i = _j\n",
    "                    break\n",
    "            else:\n",
    "                _tokens.append(__text[_i])\n",
    "                _i += 1\n",
    "\n",
    "        return _tokens\n",
    "\n",
    "    def _get_stats(self) -> Counter:\n",
    "        \"\"\"\n",
    "        Helper method to get statistics on character pairs in the current vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            A defaultdict object containing frequencies of character pairs.\n",
    "        \"\"\"\n",
    "        _pairs = defaultdict(int)\n",
    "        for _word, _freq in self.vocab.items():\n",
    "            _symbols = _word.split()\n",
    "            for i in range(len(_symbols) - 1):\n",
    "                _pairs[_symbols[i], _symbols[i + 1]] += _freq\n",
    "        return _pairs"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
