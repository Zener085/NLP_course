{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from typing import Iterable"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T16:19:31.303692Z",
     "start_time": "2024-04-01T16:19:31.298865Z"
    }
   },
   "id": "efcdc0c0ae26e751",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   class                                               text\n0     12   Rules Changed Up is the debut studio album by...\n1     14   Back is a novel written by British writer Hen...\n2     14   Love and Glory (ISBN 0-385-29261-9) is a 1983...\n3     13   Max Manus: Man of War is a 2008 Norwegian bio...\n4      7   The former Ahavas Sholem Synagogue building w...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>class</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12</td>\n      <td>Rules Changed Up is the debut studio album by...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14</td>\n      <td>Back is a novel written by British writer Hen...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>14</td>\n      <td>Love and Glory (ISBN 0-385-29261-9) is a 1983...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13</td>\n      <td>Max Manus: Man of War is a 2008 Norwegian bio...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>The former Ahavas Sholem Synagogue building w...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"data\"\n",
    "\n",
    "train_df = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "train_df.dropna(inplace=True)\n",
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T16:19:31.791528Z",
     "start_time": "2024-04-01T16:19:31.306726Z"
    }
   },
   "id": "7b938090d84165cf",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timdi\\anaconda3\\envs\\DS\\Lib\\site-packages\\torchtext\\data\\utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"spacy\")\n",
    "\n",
    "\n",
    "def preprocess_text(s):\n",
    "    s = s.strip()\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-zA-Z.,!?]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
    "    s = s.strip()\n",
    "    return s"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T16:19:32.873665Z",
     "start_time": "2024-04-01T16:19:31.795536Z"
    }
   },
   "id": "ac4ed106130040f3",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocabulary: 100%|██████████| 100800/100800 [00:14<00:00, 7007.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  200000\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(dataset):\n",
    "    for text in tqdm(dataset, desc=\"Building vocabulary\"):\n",
    "        yield tokenizer(preprocess_text(str(text)))\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    build_vocab(train_df[\"text\"].values),\n",
    "    max_tokens=200000,\n",
    "    specials=[\"<UNK>\", \"<PAD>\"],\n",
    "    special_first=True,\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print(\"Vocabulary size: \", VOCAB_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T16:20:57.124196Z",
     "start_time": "2024-04-01T16:20:42.050888Z"
    }
   },
   "id": "65523b73501cfef0",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "SEQUENCE_LENGTH = 100\n",
    "\n",
    "\n",
    "def text_pipeline(__text: str):\n",
    "    return vocab(tokenizer(preprocess_text(__text)))\n",
    "\n",
    "\n",
    "def collate_fn(__batch: Iterable):\n",
    "    texts, labels = [], []\n",
    "    for text, label in __batch:\n",
    "        text_tokens_ids = text_pipeline(text)\n",
    "        if len(text_tokens_ids) > SEQUENCE_LENGTH:\n",
    "            text_tokens_ids = text_tokens_ids[:SEQUENCE_LENGTH]\n",
    "        elif len(text_tokens_ids) < SEQUENCE_LENGTH:\n",
    "            text_tokens_ids.extend(vocab([\"<PAD>\" for _ in range(SEQUENCE_LENGTH - len(text_tokens_ids))]))\n",
    "\n",
    "        texts.append(text_tokens_ids)\n",
    "        labels.append(label - 1)\n",
    "    texts = torch.tensor(texts, dtype=torch.int)\n",
    "    labels = torch.tensor(labels, dtype=torch.float)\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "data = np.column_stack((train_df[\"text\"].values, train_df[\"class\"].values))\n",
    "train_dataloader = DataLoader(data, BATCH_SIZE, True, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T16:21:02.969058Z",
     "start_time": "2024-04-01T16:21:02.957466Z"
    }
   },
   "id": "d785f7e95bf6627d",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text, h0, c0):\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        output, (hidden, cell) = self.lstm(embedded, (h0, c0))\n",
    "        return self.fc(hidden[-1, :, :])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T16:21:03.711106Z",
     "start_time": "2024-04-01T16:21:03.703646Z"
    }
   },
   "id": "a7af3fd54d1ce2c5",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 14\n",
    "N_LAYERS = 5\n",
    "\n",
    "model = RNN(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss().to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T16:21:04.465967Z",
     "start_time": "2024-04-01T16:21:04.291256Z"
    }
   },
   "id": "5c79da6949b1a47c",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn):\n",
    "    global train_dataloader, DEVICE, N_LAYERS, HIDDEN_DIM\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for text, labels in tqdm(train_dataloader):\n",
    "        text = text.to(DEVICE)\n",
    "        labels = labels.to(DEVICE).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        h0 = torch.randn(N_LAYERS, text.shape[0], HIDDEN_DIM, device=DEVICE)\n",
    "        c0 = torch.randn(N_LAYERS, text.shape[0], HIDDEN_DIM, device=DEVICE)\n",
    "        predictions = model(text, h0, c0)\n",
    "        loss = loss_fn(predictions, labels)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return epoch_loss / len(train_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T16:21:05.265787Z",
     "start_time": "2024-04-01T16:21:05.259248Z"
    }
   },
   "id": "5fd46186bf62f214",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6300/6300 [02:26<00:00, 42.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 2.6355843584499663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6300/6300 [02:33<00:00, 40.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train Loss: 2.639563256483229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6300/6300 [02:30<00:00, 41.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train Loss: 2.48703523660463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6300/6300 [02:29<00:00, 42.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train Loss: 1.4151550195803717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6300/6300 [02:32<00:00, 41.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train Loss: 0.46779079507932897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6300/6300 [02:32<00:00, 41.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train Loss: 0.18303175392738055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6300/6300 [02:32<00:00, 41.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train Loss: 0.10602147062330337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6300/6300 [02:31<00:00, 41.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train Loss: 0.07425418831260194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6300/6300 [02:32<00:00, 41.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train Loss: 0.05457411956789512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6300/6300 [02:45<00:00, 38.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train Loss: 0.042264396312999046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train(model, optimizer, loss_fn)\n",
    "    print(f\"Epoch: {epoch}, Train Loss: {train_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T16:46:33.897525Z",
     "start_time": "2024-04-01T16:21:05.944167Z"
    }
   },
   "id": "fbb78c44ce969737",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(f\"{data_path}/test.csv\")\n",
    "test_data = np.column_stack((test_df[\"text\"].values, np.zeros(test_df[\"text\"].shape)))\n",
    "test_dataloader = DataLoader(test_data, BATCH_SIZE, collate_fn=collate_fn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T16:50:35.112907Z",
     "start_time": "2024-04-01T16:50:35.049139Z"
    }
   },
   "id": "a5761e8ba683c609",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def predict(model):\n",
    "    global test_dataloader\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text, _ in tqdm(test_dataloader):\n",
    "            text = text.to(DEVICE)\n",
    "\n",
    "            h0 = torch.zeros(N_LAYERS, text.size(0), HIDDEN_DIM).to(DEVICE)\n",
    "            c0 = torch.zeros(N_LAYERS, text.size(0), HIDDEN_DIM).to(DEVICE)\n",
    "\n",
    "            output = model(text, h0, c0)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            predicted += 1\n",
    "            predictions.extend(predicted.tolist())\n",
    "\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T16:50:35.817185Z",
     "start_time": "2024-04-01T16:50:35.811558Z"
    }
   },
   "id": "cf18cc5de0cecdbc",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 700/700 [00:07<00:00, 88.50it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(model)\n",
    "\n",
    "submission_df = pd.DataFrame({\"id\": test_df[\"id\"], \"class_id\": predictions})\n",
    "submission_df.to_csv(\"submission.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-01T16:50:44.409693Z",
     "start_time": "2024-04-01T16:50:36.476363Z"
    }
   },
   "id": "f96cf7caa9c3c790",
   "execution_count": 32
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
